@misc{3b1b_nn,
  title = {Neural {{Networks}}},
  author = {{3Blue1Brown}},
  year = {2024},
  month = aug,
  journal = {3blue1brown.com},
  urldate = {2024-12-13},
  howpublished = {https://www.3blue1brown.com/topics/neural-networks},
  langid = {american}
}

@misc{adaptive_model2024,
  title = {Navigating {{Scaling Laws}}: {{Compute Optimality}} in {{Adaptive Model Training}}},
  shorttitle = {Navigating {{Scaling Laws}}},
  author = {Anagnostidis, Sotiris and Bachmann, Gregor and Schlag, Imanol and Hofmann, Thomas},
  year = {2024},
  month = may,
  number = {arXiv:2311.03233},
  eprint = {2311.03233},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03233},
  urldate = {2024-12-11},
  abstract = {In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. We show that our approach generalizes across modalities and different shape parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/sergiogabardo/Zotero/storage/8U8I99BY/Anagnostidis et al. - 2024 - Navigating Scaling Laws Compute Optimality in Adaptive Model Training.pdf;/Users/sergiogabardo/Zotero/storage/2E8AGJL5/2311.html}
}

@misc{adobeAdobeFirefly2023,
  title = {Adobe {{Firefly}}},
  author = {{Adobe}},
  year = {2023},
  journal = {Adobe Firefly},
  urldate = {2023-09-21},
  abstract = {Use generative AI and simple text prompts to create the highest-quality output --- beautiful images, text effects, and fresh color palettes. Make all-new content from reference images and explore more possibilities, more quickly.},
  howpublished = {https://www.adobe.com/sensei/generative-ai/firefly.html}
}

@misc{aeroSMOKERSCORNERCONSTANTS,
  title = {{{SMOKERS}}' {{CORNER}}: {{THE CONSTANTS OF SCAPEGOATING}}},
  author = {{Aero}},
  urldate = {2023-11-18}
}

@book{aima2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2021},
  series = {Pearson {{Series}} in {{Artificial Intelligence}}},
  edition = {Fourth Edition},
  publisher = {Pearson},
  address = {Hoboken, NJ},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  collaborator = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Wooldridge, Michael J.},
  isbn = {978-0-13-461099-3},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/X8N2JIEP/Russell and Norvig - 2021 - Artificial intelligence a modern approach.pdf}
}

@misc{amazonwebservicesReducingMachineLearning2020,
  title = {Reducing {{Machine Learning Inference Cost}} for {{PyTorch Models}} - {{AWS Online Tech Talks}}},
  author = {{Amazon Web Services}},
  year = {2020},
  journal = {AWS Developers},
  urldate = {2024-01-01}
}

@misc{ambroseThoreauFDR2015,
  title = {Thoreau \& {{FDR}}},
  author = {Ambrose, Lorell},
  year = {2015},
  month = jan,
  journal = {The Thoreau Society},
  urldate = {2023-11-13},
  abstract = {In a recent article in the Chicago Tribune, "10 Things You Might Not Know about Fear," Mark Jacob and Stephan Benzkofer mentioned the relatively obscure},
  langid = {american},
  file = {/Users/sergiogabardo/Zotero/storage/GWCJS84G/thoreau-fdr.html}
}

@misc{americanpsychologicalassociationScapegoating2018,
  title = {Scapegoating},
  author = {{American Psychological Association}},
  year = {2018},
  month = apr,
  journal = {APA Dictionary of Psychology},
  publisher = {American Psychological Association},
  address = {Washington, DC},
  urldate = {2023-11-17},
  abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/GK72WSRD/scapegoating.html}
}

@misc{asam2021,
  title = {{{ASAM}}: {{Adaptive Sharpness-Aware Minimization}} for {{Scale-Invariant Learning}} of {{Deep Neural Networks}}},
  shorttitle = {{{ASAM}}},
  author = {Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  year = {2021},
  month = jun,
  number = {arXiv:2102.11600},
  eprint = {2102.11600},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.11600},
  urldate = {2024-12-11},
  abstract = {Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-ofthe-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scaleinvariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{attention2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1706.03762},
  urldate = {2024-12-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{behavioral2007,
  title = {Early Human Use of Marine Resources and Pigment in {{South Africa}} during the {{Middle Pleistocene}}},
  author = {Marean, Curtis W. and {Bar-Matthews}, Miryam and Bernatchez, Jocelyn and Fisher, Erich and Goldberg, Paul and Herries, Andy I. R. and Jacobs, Zenobia and Jerardino, Antonieta and Karkanas, Panagiotis and Minichillo, Tom and Nilssen, Peter J. and Thompson, Erin and Watts, Ian and Williams, Hope M.},
  year = {2007},
  month = oct,
  journal = {Nature},
  volume = {449},
  number = {7164},
  pages = {905--908},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06204},
  urldate = {2024-12-11},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@article{brossePromisesPitfallsStochastic2018,
  title = {The Promises and Pitfalls of {{Stochastic Gradient Langevin Dynamics}}},
  author = {Brosse, Nicolas and Durmus, Alain and Moulines, Eric},
  year = {2018},
  journal = {32nd Conference on Neural Information Processing Systems},
  doi = {10.48550/arXiv.1811.10072},
  urldate = {2024-01-01}
}

@article{bundea2024,
  title = {Pneumonia {{Image Classification Using DenseNet Architecture}}},
  author = {Bundea, Mihai and Danciu, Gabriel Mihail},
  year = {2024},
  month = oct,
  journal = {Information},
  volume = {15},
  number = {10},
  pages = {611},
  issn = {2078-2489},
  doi = {10.3390/info15100611},
  urldate = {2024-12-11},
  abstract = {Pulmonary diseases, including pneumonia, represent a significant health challenge and are often diagnosed using X-rays. This study investigates the effectiveness of artificial intelligence (AI) in enhancing the diagnostic capabilities of X-ray imaging. Using Python and the PyTorch framework, we developed and trained several deep learning models based on DenseNet architectures (DenseNet121, DenseNet169, and DenseNet201) on a dataset comprising 5856 annotated X-ray images classified into two categories: Normal (Healthy) and Pneumonia. Each model was evaluated on its ability to classify images with metrics including binary accuracy, sensitivity, and specificity. The results demonstrated accuracy rates of 92\% for Normal and 97\% for Pneumonia. The models also showed significant improvements in diagnostic accuracy and reduced time for disease detection compared to traditional methods. This study underscores the potential of integrating convolutional neural networks (CNNs) with medical imaging to enhance diagnostic precision and support clinical decision-making in the management of pulmonary diseases. Further research is encouraged to refine these models and explore their application in other medical imaging domains.},
  copyright = {CC-BY-4.0},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/JEF89L6X/Bundea and Danciu - 2024 - Pneumonia Image Classification Using DenseNet Architecture.pdf}
}

@misc{chatgpt2022,
  title = {Introducing {{ChatGPT}}},
  author = {{OpenAI}},
  year = {2022},
  month = nov,
  urldate = {2024-12-05},
  howpublished = {https://openai.com/index/chatgpt/},
  langid = {english}
}

@article{chen2024,
  title = {A {{Lightweight Barcode Detection Algorithm Based}} on {{Deep Learning}}},
  author = {Chen, Jingchao and Dai, Ning and Hu, Xudong and Yuan, Yanhong},
  year = {2024},
  month = nov,
  journal = {Applied Sciences},
  volume = {14},
  number = {22},
  pages = {10417},
  issn = {2076-3417},
  doi = {10.3390/app142210417},
  urldate = {2024-12-05},
  abstract = {For existing situations of missed detections, false detections, and repeated detections in barcode detection algorithms in real-world scenarios, a barcode detection algorithm based on improved YOLOv8 is proposed. The EfficientViT block based on a linear self-attention mechanism is introduced into the backbone of the original model to enhance the model's attention to barcode features. In the model's neck, linear mapping and grouped convolution are used to improve the C2f module, and the ADown convolution block is utilized to modify the model's downsampling, which reduces the model's parameters and computational cost while improving the efficiency of model feature fusion. Finally, the reconstruction of the model's detection head and the modification of the loss function are implemented to enhance the model's training quality and reduce the model's error in barcode detection. Experimental results indicate that the improved model exhibits an increase of 1.8\% in recall rate and 1.9\% in mAP50:95 for barcode localization and classification. The FPS is improved by 40 frames per second. The model's parameter count is reduced by 74.2\%, and FLOPs are decreased by 79.6\%. Furthermore, the proposed model outperforms other models in terms of model size and barcode detection accuracy.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/XQUPDSRM/Chen et al. - 2024 - A Lightweight Barcode Detection Algorithm Based on Deep Learning.pdf}
}

@misc{cifar10,
  type = {Dataset},
  title = {{{CIFAR-10 Dataset}}},
  shorttitle = {{{CIFAR-10}}},
  author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  year = {2009},
  number = {cifar10},
  publisher = {University of Toronto},
  address = {https://www.cs.toronto.edu/{\textasciitilde}kriz/cifar.html},
  urldate = {2024-12-16},
  copyright = {Free for academic use},
  file = {/Users/sergiogabardo/Zotero/storage/8778ZXL4/cifar.html}
}

@misc{cifar100,
  type = {Dataset},
  title = {{{CIFAR-100 Dataset}}},
  shorttitle = {{{CIFAR-10}}},
  author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  year = {2009},
  number = {cifar100},
  publisher = {University of Toronto},
  address = {https://www.cs.toronto.edu/{\textasciitilde}kriz/cifar.html},
  urldate = {2024-12-16},
  copyright = {Free for academic use}
}

@article{cnn1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2024-12-11},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english}
}

@misc{cnnbrasilComoFoiGolpe2023,
  title = {Como Foi o Golpe No {{Chile}} Em 1973? {{Relembre}} Origem, Respons{\'a}veis \hspace{0pt}\hspace{0pt}e Consequ{\^e}ncias},
  shorttitle = {Como Foi o Golpe No {{Chile}} Em 1973?},
  author = {{CNN Brasil}},
  year = {2023},
  month = sep,
  journal = {CNN Brasil},
  urldate = {2023-11-13},
  abstract = {H{\'a} exatos 50 anos, em 11 de setembro, o general Augusto Pinochet tomou o poder no pa{\'i}s e instalou um regime militar que executou 3.200 pessoas em 17 anos},
  howpublished = {https://www.cnnbrasil.com.br/internacional/como-foi-o-golpe-no-chile-em-1973-relembre-origem-responsaveis-e-consequencias/},
  file = {/Users/sergiogabardo/Zotero/storage/5S2VT986/como-foi-o-golpe-no-chile-em-1973-relembre-origem-responsaveis-e-consequencias.html}
}

@article{desislavovTrendsAIInference2023,
  title = {Trends in {{AI}} Inference Energy Consumption: {{Beyond}} the Performance-vs-Parameter Laws of Deep Learning},
  author = {Desislavov, Radosvet and {Mart{\'i}nez-Plumed}, Fernando and {Hern{\'a}ndez-Orallo}, Jos{\'e}},
  year = {2023},
  journal = {Sustainable Computing: Informatics and Systems},
  volume = {38},
  pages = {100857},
  doi = {10.1016/j.suscom.2023.100857},
  urldate = {2024-01-01}
}

@misc{douwes2024,
  title = {From {{Computation}} to {{Consumption}}: {{Exploring}} the {{Compute-Energy Link}} for {{Training}} and {{Testing Neural Networks}} for {{SED Systems}}},
  shorttitle = {From {{Computation}} to {{Consumption}}},
  author = {Douwes, Constance and Serizel, Romain},
  year = {2024},
  month = sep,
  number = {arXiv:2409.05080},
  eprint = {2409.05080},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05080},
  urldate = {2024-12-11},
  abstract = {The massive use of machine learning models, particularly neural networks, has raised serious concerns about their environmental impact. Indeed, over the last few years we have seen an explosion in the computing costs associated with training and deploying these systems. It is, therefore, crucial to understand their energy requirements in order to better integrate them into the evaluation of models, which has so far focused mainly on performance. In this paper, we study several neural network architectures that are key components of sound event detection systems, using an audio tagging task as an example. We measure the energy consumption for training and testing small to large architectures and establish complex relationships between the energy consumption, the number of floating-point operations, the number of parameters, and the GPU/memory utilization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/Users/sergiogabardo/Zotero/storage/MJKBCVWY/Douwes and Serizel - 2024 - From Computation to Consumption Exploring the Compute-Energy Link for Training and Testing Neural N.pdf;/Users/sergiogabardo/Zotero/storage/LWD8G4Z3/2409.html}
}

@techreport{dtic1978,
  type = {Technical {{Report}}},
  title = {An {{Overview}} of {{Optical Character Recognition}} ({{OCR}}) {{Technology}} and {{Techniques}}},
  author = {{L. K. Gronmeyer} and {B. W. Ruffin} and {M. A. Lybanon} and {S. E. Pierce, Jr.} and {P. L. Neely}},
  year = {1978},
  month = jun,
  number = {ADA131341},
  address = {NSTL Station, Mississippi},
  institution = {{Naval Ocean Research and Development Activity (NORDA)}},
  urldate = {2024-12-05},
  langid = {american},
  lccn = {ADA131341},
  file = {/Users/sergiogabardo/Zotero/storage/58VFGSFF/L. K. Gronmeyer et al. - 1978 - An Overview of Optical Character Recognition (OCR) Technology and Techniques.pdf}
}

@misc{ElisabethVigeeLebrunPortrait,
  title = {{\'E}lisabeth {{Vig{\'e}e-Lebrun}}: Portrait of {{Marie-Antoinette}}},
  urldate = {2023-11-14},
  abstract = {{\'E}lisabeth Vig{\'e}e-Lebrun: portrait of Marie-Antoinette},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/F3RDADB7/Marie-Antoinette-queen-of-France.html}
}

@techreport{eloundouGPTsAreGPTs2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  year = {2023},
  month = aug,
  institution = {University of Pennsylvania},
  abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre- trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
  langid = {american}
}

@misc{fig-gradient_vectorfield,
  title = {3d-Gradient-Cos.Svg},
  author = {{MartinThoma}},
  year = {2018},
  month = aug,
  urldate = {2024-12-13},
  copyright = {CC0},
  file = {/Users/sergiogabardo/Zotero/storage/TB3PYFAR/3d-gradient-cos.svg}
}

@misc{fig-overfitting,
  type = {Media {{Repository}}},
  title = {Pyplot Overfitting},
  author = {{ThirdOrderLogic}},
  year = {2024},
  month = jul,
  journal = {Wikimedia},
  urldate = {2024-12-11},
  copyright = {CC-BY-4.0},
  howpublished = {https://commons.wikimedia.org/wiki/File:Pyplot\_overfitting.png}
}

@misc{fig-yennhi95zz_evolution,
  title = {Understanding the {{Cost Function}} in {{Linear Regression}} for {{Machine Learning Beginners}}},
  author = {Nhi, Yen},
  year = {2023},
  month = may,
  urldate = {2024-12-11},
  langid = {english}
}

@misc{FranklinRoosevelt,
  title = {Franklin {{D}}. {{Roosevelt}}},
  journal = {The White House},
  urldate = {2023-11-13},
  abstract = {Assuming the Presidency at the depth of the Great Depression as our 32nd President (1933-1945), Franklin D. Roosevelt helped the American people regain faith in themselves.},
  copyright = {United States Government},
  howpublished = {https://www.whitehouse.gov/about-the-white-house/presidents/franklin-d-roosevelt/},
  langid = {american},
  file = {/Users/sergiogabardo/Zotero/storage/LWGMETDY/franklin-d-roosevelt.html}
}

@misc{fusterAmericaSulGuerra2018,
  title = {{Am{\'e}rica do Sul e a Guerra Fria: Inger{\^e}ncia Norte Americana}},
  author = {Fuster, Danilo Andr{\'e}},
  year = {2018},
  month = may,
  journal = {Escola de Contas TCM/SP},
  urldate = {2023-11-13},
  howpublished = {https://escoladecontas.tcm.sp.gov.br/artigos/1295-america-do-sul-e-a-guerra-fria-ingerencia-norte-americana},
  langid = {brazilian},
  file = {/Users/sergiogabardo/Zotero/storage/IZKPM9QP/1295-america-do-sul-e-a-guerra-fria-ingerencia-norte-americana.html}
}

@article{garretonm.RedemocratizacaoNoChile1992,
  title = {{A redemocratiza{\c c}{\~a}o no Chile: transi{\c c}{\~a}o, inaugura{\c c}{\~a}o e evolu{\c c}{\~a}o}},
  shorttitle = {{A redemocratiza{\c c}{\~a}o no Chile}},
  author = {Garret{\'o}n M., Manuel Antonio},
  year = {1992},
  month = dec,
  journal = {Lua Nova: Revista de Cultura e Pol{\'i}tica},
  volume = {27},
  pages = {59--92},
  publisher = {CEDEC},
  doi = {10.1590/S0102-64451992000300004},
  urldate = {2023-11-13},
  langid = {brazilian},
  file = {/Users/sergiogabardo/Zotero/storage/4WQ4GX6L/M and Antonio - 1992 - A redemocratização no Chile transição, inauguração e evolução.pdf}
}

@article{gergenChallengeAbsentPresence2002,
  title = {The Challenge of Absent Presence},
  author = {Gergen, Kenneth J.},
  editor = {Katz, James E. and Aakhus, Mark},
  year = {2002},
  journal = {Perpetual Contact: Mobile Communication, Private Talk, Public Performance},
  pages = {227--241},
  doi = {10.1017/cbo9780511489471.018},
  urldate = {2023-10-03}
}

@patent{goldberg1927,
  title = {Statistical {{Machine}}},
  author = {Goldberg, Emanuel},
  year = {1931},
  month = dec,
  number = {US1838389A},
  address = {Dresden},
  urldate = {2024-12-05},
  langid = {english},
  nationality = {Germany},
  file = {/Users/sergiogabardo/Zotero/storage/4HJ4SUH4/Goldberg - Statistical Machine.pdf}
}

@misc{higaDitaduraMilitarChilena,
  title = {Ditadura Militar Chilena},
  author = {Higa, Carlos C{\'e}sar},
  journal = {Mundo Educa{\c c}{\~a}o},
  urldate = {2023-11-12},
  howpublished = {https://mundoeducacao.uol.com.br/historia-america/ditadura-militar-chilena.htm},
  file = {/Users/sergiogabardo/Zotero/storage/P8L52KZF/ditadura-militar-chilena.html}
}

@misc{hough1962,
  title = {Hough {{Transform}}},
  author = {Fisher, R and Perkins, S and Walker, A and Wolfart, E},
  year = {2003},
  journal = {Hypermedia Image Processing Reference 2},
  urldate = {2024-12-12},
  howpublished = {https://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm},
  file = {/Users/sergiogabardo/Zotero/storage/GMSCICYS/hough.html}
}

@patent{hough1962_patent,
  title = {Method and Means for Recognizing Complex Patterns},
  author = {Hough, Paul V. C.},
  year = {1962},
  month = dec,
  number = {US3069654A},
  urldate = {2024-12-11},
  assignee = {Individual},
  nationality = {US},
  keywords = {framelet,line,microsecond,pulse,segment},
  file = {/Users/sergiogabardo/Zotero/storage/YYXGLJ76/Hough - 1962 - Method and means for recognizing complex patterns.pdf}
}

@misc{huntScapegoat1854,
  title = {The {{Scapegoat}}},
  author = {Hunt, William Holman},
  year = {1854},
  urldate = {2023-11-13},
  abstract = {The Scapegoat (c. 1854-1856). Oil on canvas, 86 {\texttimes} 140 cm (34 {\texttimes} 55 in). Lady Lever Art Gallery, Port Sunlight, United Kingdom},
  lccn = {LL 3623}
}

@misc{imagenet,
  type = {Dataset},
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  shorttitle = {{{ImageNet}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  number = {imagenet},
  publisher = {Stanford Vision Lab},
  address = {http://www.image-net.org},
  urldate = {2024-12-16},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  file = {/Users/sergiogabardo/Zotero/storage/9MZ3VGUS/Deng et al. - ImageNet A Large-Scale Hierarchical Image Database.pdf}
}

@misc{interactive_maml,
  title = {An {{Interactive Introduction}} to {{Model-Agnostic Meta-Learning}}},
  author = {{Luis M{\"u}ller} and {Max Ploner} and {Thomas Goerttler}},
  year = {2021},
  month = oct,
  journal = {Interactive MAML},
  urldate = {2024-12-13},
  collaborator = {{Klaus Obermayer}},
  howpublished = {https://interactive-maml.github.io/},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/C6KZHN78/interactive-maml.github.io.html}
}

@book{kahneman2011,
  title = {Thinking, Fast and Slow},
  author = {Kahneman, Daniel},
  year = {2012},
  series = {Penguin Psychology},
  publisher = {Penguin Books},
  address = {London},
  isbn = {978-0-14-103357-0},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/VGTAHI8E/Kahneman - 2012 - Thinking, fast and slow.pdf}
}

@article{krizhevsky2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  month = apr,
  urldate = {2024-12-16},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/PSJHG4VS/Krizhevsky - Learning Multiple Layers of Features from Tiny Images.pdf}
}

@article{kushlevSmartphonesReduceSmiles2019,
  title = {Smartphones Reduce Smiles between Strangers},
  author = {Kushlev, Kostadin and Hunter, John F. and Proulx, Jason and Pressman, Sarah D. and Dunn, Elizabeth},
  year = {2019},
  month = feb,
  journal = {Computers in Human Behavior},
  volume = {91},
  pages = {12--16},
  doi = {10.1016/j.chb.2018.09.023},
  urldate = {2023-10-03}
}

@article{leopoldAWSOfferNvidias2019,
  title = {{{AWS}} to {{Offer Nvidia}}'s {{T4 GPUs}} for {{AI Inferencing}}},
  author = {Leopold, George and Trader, Tiffany},
  year = {2019},
  journal = {HPCwire},
  publisher = {Tabor Communications},
  urldate = {2024-01-01}
}

@article{limaMemoriasEmConstrucao2019,
  title = {{Mem{\'o}rias em constru{\c c}{\~a}o: o presente e o passado da ditadura militar chilena representados no Museo de la Memoria y los Derechos Humanos}},
  shorttitle = {{Mem{\'o}rias em constru{\c c}{\~a}o}},
  author = {Lima, Fernanda Lu{\'i}za Teixeira and {de Carvalho}, Aline Vieira},
  year = {2019},
  month = apr,
  journal = {Horizontes Antropol{\'o}gicos},
  volume = {25},
  pages = {81--105},
  publisher = {Programa de P{\'o}s-Gradua{\c c}{\~a}o em Antropologia Social - IFCH-UFRGS},
  doi = {10.1590/S0104-71832019000100004},
  urldate = {2023-11-13},
  abstract = {Nos {\'u}ltimos vinte cinco anos, nos pa{\'i}ses do Cone Sul, o debate sobre as formas de recorda{\c c}{\~a}o sobre o passado ditatorial incluiu um vasto processo de memorializa{\c c}{\~a}o caracterizado pela constru{\c c}{\~a}o de espa{\c c}os museol{\'o}gicos que recordam as viola{\c c}{\~o}es dos direitos humanos perpetrados pela ditadura civil-militar. Grande parte da revitaliza{\c c}{\~a}o e constru{\c c}{\~a}o desses espa{\c c}os foi liderada por alguns setores da sociedade civil, que ergueram monumentos e memoriais em homenagem {\`a}s v{\'i}timas, e tamb{\'e}m recuperaram antigos centros de tortura transformando-os em espa{\c c}os museol{\'o}gicos. Nesse contexto marcado por conflitos e negocia{\c c}{\~o}es, o patrim{\^o}nio assume diversas formas e fun{\c c}{\~o}es, ora {\'e} configurado como prolongamento do poder e viol{\^e}ncia, ora como representa{\c c}{\~a}o de um caminho de repara{\c c}{\~a}o e reconcilia{\c c}{\~a}o. Buscando compreender o movimento de mobiliza{\c c}{\~a}o desse passado no presente, este artigo pretende fazer uma an{\'a}lise da obra Geometr{\'i}a de la conciencia, de Alfredo Jaar, e da exposi{\c c}{\~a}o permanente do Museo de la Memoria y los Derechos Humanos, no Chile, institui{\c c}{\~a}o fruto da iniciativa governamental e civil, tendo como foco a an{\'a}lise de dois eixos espec{\'i}ficos: a mem{\'o}ria da repress{\~a}o e o lugar das v{\'i}timas da ditadura militar chilena.},
  langid = {spanish},
  keywords = {Chile,ditadura,lugar de memoria,memoria},
  file = {/Users/sergiogabardo/Zotero/storage/J33J857Y/Lima and Carvalho - 2019 - Memórias em construção o presente e o passado da ditadura militar chilena representados no Museo de.pdf}
}

@article{loquercioGeneralFrameworkUncertainty2020,
  title = {A {{General Framework}} for {{Uncertainty Estimation}} in {{Deep Learning}}},
  author = {Loquercio, Antonio and Segu, Mattia and Scaramuzza, Davide},
  year = {2020},
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {3153--3160},
  doi = {10.1109/LRA.2020.2974682},
  urldate = {2024-01-01}
}

@misc{media4mathTINspireCXMiniTutorial2011,
  title = {{{TI-Nspire CX Mini-Tutorial}}: {{Binomial Distribution}}},
  shorttitle = {{{TI-Nspire CX Mini-Tutorial}}},
  author = {{Media4Math}},
  year = {2011},
  month = oct,
  urldate = {2023-11-20},
  abstract = {Learn about binomial distributions using the TI-Nspire graphing calculator. This is part of a library of mini-tutorial videos for using the TI-Nspire CX. https://www.media4math.com/TI-Resources}
}

@misc{mondalFishermanPainting2021,
  title = {Fisherman {{Painting}}},
  author = {Mondal, Shankhadeep},
  year = {2021},
  urldate = {2023-11-14},
  file = {/Users/sergiogabardo/Zotero/storage/DU2EX9P9/view.html}
}

@article{mullerInteractiveIntroductionModelAgnostic2021,
  title = {An {{Interactive Introduction}} to {{Model-Agnostic Meta-Learning}}},
  author = {M{\"u}ller, Luis and Ploner, Max and Goerttler, Thomas and Obermayer, Klaus},
  year = {2021},
  urldate = {2024-01-01}
}

@misc{openaiDALLE32023,
  title = {{{DALL}}{$\cdot$}{{E}} 3},
  author = {{OpenAI}},
  year = {2023},
  journal = {OpenAI},
  urldate = {2023-01-02},
  howpublished = {https://openai.com/dall-e-3}
}

@article{overfitting2004,
  title = {The {{Problem}} of {{Overfitting}}},
  author = {Hawkins, Douglas M.},
  year = {2003},
  month = dec,
  journal = {American Chemical Society},
  volume = {44},
  number = {1},
  pages = {1--12},
  doi = {10.1021/ci0342472},
  urldate = {2024-12-11},
  langid = {english}
}

@misc{perskieOfficialCampaignColor1944,
  title = {Official Campaign Color Portrait of {{U}}.{{S}}. {{President Franklin Delano Roosevelt}}, 1944},
  shorttitle = {{{FDR}} 1944 {{Color Portrait}}},
  author = {Perskie, Leon},
  year = {1944},
  month = aug,
  urldate = {2023-11-13},
  abstract = {Original color transparency of FDR taken at 1944 Official Campaign Portrait session by Leon A. Perskie, Hyde Park, New York, August 22, 1944. Gift of Beatrice Perskie Foxman and Dr. Stanley B. Foxman.},
  file = {/Users/sergiogabardo/Zotero/storage/W9843XKW/FileFDR_1944_Color_Portrait.html}
}

@misc{razzettiHowWinBlame2018,
  title = {How to {{Win}} the {{Blame Game}}: {{Stop Playing}}},
  author = {Razzetti, Gustavo},
  year = {2018},
  month = mar,
  journal = {Linkedin},
  urldate = {2023-11-14},
  howpublished = {https://www.linkedin.com/pulse/how-win-blame-game-stop-playing-gustavo/},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/6KIAG95S/how-win-blame-game-stop-playing-gustavo.html}
}

@article{riordanScapegoatMechanismHuman,
  title = {The {{Scapegoat Mechanism}} in {{Human Evolution}}: {{An Analysis}} of {{Ren{\'e} Girard}}'s {{Hypothesis}} on the {{Process}} of {{Hominization}}},
  author = {Riordan, D. Vincent},
  volume = {16},
  pages = {242--256},
  doi = {10.1007/s13752-021-00381-y}
}

@article{riordanScapegoatMechanismHuman2021,
  title = {The {{Scapegoat Mechanism}} in {{Human Evolution}}: {{An Analysis}} of {{Ren{\'e} Girard}}'s {{Hypothesis}} on the {{Process}} of {{Hominization}}},
  author = {Riordan, D. Vincent},
  year = {2021},
  month = dec,
  journal = {Biological Theory},
  volume = {16},
  number = {4},
  pages = {242--256},
  issn = {1555-5550},
  doi = {10.1007/s13752-021-00381-y},
  abstract = {According to anthropological philosopher Ren{\'e} Girard (1923--2015), an important human adaptation is our propensity to victimize or scapegoat. He argued that other traits upon which human sociality depends would have destabilized primate dominance-based social hierarchies, making conspecific conflict a limiting factor in hominin evolution. He surmised that a novel mechanism for inhibiting intragroup conflict must have emerged contemporaneously with our social traits, and speculated that this was the tendency to spontaneously unite around the victimization of single individuals. He described an unconscious tendency to both ascribe blame and to imbue the accused with a sacred mystique. This emotionally cathartic scapegoat mechanism, he claimed, enhanced social cohesion, and was the origin of religion, mythology, sacrifice, ritual, cultural institutions, and social norms. It would have functioned by modifying the beliefs and behaviors of the group, rather than of the accused, making the act of accusation more important than the substance. This article aims to examine the empirical evidence for Girard's claims, and argues that the scapegoat hypothesis has commonalities with several other evolutionary hypotheses, including Wrangham's execution hypothesis on self-domestication, Dunbar's hypothesis on the role of storytelling in maintaining group stability, and DeScioli and Kurzban's hypothesis on the role of non-consequentialist morality in curtailing conflict. Potential implications of the scapegoat hypothesis for evolutionary psychology and psychiatry are discussed.}
}

@misc{RLsota2024,
  title = {Reinforcement {{Learning}} - {{My Algorithm}} vs {{State}} of the {{Art}}},
  author = {Jean, Tampon},
  year = {2024},
  month = nov,
  volume = {3},
  urldate = {2024-12-11},
  langid = {english}
}

@article{rooseAIGeneratedPictureWon2022,
  title = {An {{A}}.{{I}}.-{{Generated Picture Won}} an {{Art Prize}}. {{Artists Aren}}'t {{Happy}}.},
  author = {Roose, Kevin},
  year = {2022},
  journal = {The New York Times},
  chapter = {Technology}
}

@misc{roosevelt1933InauguralAddress1933,
  title = {1933 {{Inaugural Address}}},
  author = {Roosevelt, Franklin Delano},
  year = {1933},
  month = mar,
  urldate = {2023-11-13},
  langid = {american}
}

@book{russelArtificialIntelligenceModern2020,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  author = {Russel, Stuart and Norvig, Peter},
  year = {2020},
  edition = {4},
  publisher = {Pearson},
  isbn = {978-0-13-461099-3}
}

@misc{saderDitadurasMilitares,
  title = {{Ditaduras Militares}},
  author = {Sader, Emir},
  journal = {Portal Contempor{\^a}neo da Am{\'e}rica Latina e Caribe - USP},
  urldate = {2023-11-12},
  howpublished = {https://sites.usp.br/portalatinoamericano/espanol-dictaduras-militares},
  langid = {brazilian},
  file = {/Users/sergiogabardo/Zotero/storage/VDJ4MYWB/espanol-dictaduras-militares.html}
}

@misc{salkowitzMidjourneyFounderDavid2022,
  title = {Midjourney {{Founder David Holz On The Impact Of AI On Art}}, {{Imagination And The Creative Economy}}},
  author = {Salkowitz, Rob},
  year = {2022},
  month = sep,
  journal = {Forbes},
  howpublished = {https://www.forbes.com/sites/robsalkowitz/2022/09/16/midjourney-founder-david-holz-on-the-impact-of-ai-on-art-imagination-and-the-creative-economy},
  langid = {american}
}

@misc{sam2020,
  title = {Sharpness-{{Aware Minimization}} for {{Efficiently Improving Generalization}}},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2021},
  month = apr,
  number = {arXiv:2010.01412},
  eprint = {2010.01412},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.01412},
  urldate = {2024-12-11},
  abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at {\textbackslash}url\{https://github.com/google-research/sam\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/sergiogabardo/Zotero/storage/DF2499FL/Foret et al. - 2021 - Sharpness-Aware Minimization for Efficiently Improving Generalization.pdf;/Users/sergiogabardo/Zotero/storage/KG3LT29T/2010.html}
}

@misc{scaling_laws2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2024-12-11},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/sergiogabardo/Zotero/storage/JY83K5DA/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/sergiogabardo/Zotero/storage/L5NJ7NRR/2001.html}
}

@misc{ScapegoatArtUK,
  title = {The {{Scapegoat}} {\textbar} {{Art UK}}},
  urldate = {2023-11-17},
  abstract = {The Scapegoat by William Holman Hunt (1827--1910), 1854--1855, from Manchester Art Gallery},
  howpublished = {https://artuk.org/discover/artworks/the-scapegoat-205247},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/WHVHKP8T/the-scapegoat-205247.html}
}

@misc{Scapegoating,
  title = {Scapegoating}
}

@misc{SergioMiguelGabardo,
  title = {S{\'e}rgio {{Miguel Gabardo}} - {{BSS CINECLUBE}} --- {{AN{\'A}LISE FILMOGR{\'A}FICA DA OBRA}} ``{{MACHUCA}}'' --- {{ATIVIDADE SOMATIVA}}},
  journal = {Google Docs},
  urldate = {2023-11-13},
  howpublished = {https://docs.google.com/document/u/0/d/1z8WKLZOM1KQBsiKVOcDQWkZ\_dytMahtyjI8ah3xQZgw/edit?usp=drive\_web\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/I4ISQRQE/edit.html}
}

@article{shannon1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C. E.},
  year = {1948},
  month = oct,
  journal = {The Bell System Technical Journal},
  volume = {27},
  pages = {379--423, 623--656},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  urldate = {2024-12-12},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/4KCPSHHI/Shannon - A Mathematical Theory of Communication.pdf}
}

@article{shannon1951,
  title = {Prediction and {{Entropy}} of {{Printed English}}},
  author = {Shannon, C E},
  year = {1951},
  month = jan,
  journal = {The Bell System Technical Journal},
  langid = {english},
  file = {/Users/sergiogabardo/Zotero/storage/HEBX27MV/Shannon - Prediction and Entropy of Printed English.pdf}
}

@misc{SOTApwcIC,
  title = {Image {{Classification}} on {{CIFAR-100}}},
  author = {{Papers With Code}},
  year = {2024},
  urldate = {2024-12-12},
  copyright = {CC-BY-SA},
  howpublished = {https://paperswithcode.com/sota/image-classification-on-cifar-100}
}

@misc{sustainableAI2022,
  title = {Sustainable {{AI}}: {{Environmental Implications}}, {{Challenges}} and {{Opportunities}}},
  shorttitle = {Sustainable {{AI}}},
  author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Behram, Fiona Aga and Huang, James and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin S. and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},
  year = {2022},
  month = jan,
  number = {arXiv:2111.00364},
  eprint = {2111.00364},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.00364},
  urldate = {2024-12-11},
  abstract = {This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  file = {/Users/sergiogabardo/Zotero/storage/Z55GQPPC/Wu et al. - 2022 - Sustainable AI Environmental Implications, Challenges and Opportunities.pdf;/Users/sergiogabardo/Zotero/storage/YNP865WW/2111.html}
}

@misc{suttieHowPhonesCompromise2019,
  title = {How {{Phones Compromise Our Ability}} to {{Connect}}},
  author = {Suttie, Jill},
  year = {2019},
  month = jan,
  publisher = {Greater Good Magazine},
  urldate = {2023-10-03}
}

@misc{tesseract2006,
  title = {Tesseract: An {{Open-Source Optical Character Recognition Engine}}},
  author = {Kay, Anthony},
  year = {2007},
  month = jul,
  journal = {Linux Journal},
  urldate = {2024-12-12},
  howpublished = {https://www.linuxjournal.com/article/9676}
}

@techreport{u.s.copyrightofficeCompendiumUSCopyright2021,
  title = {Compendium of {{U}}.{{S}}. {{Copyright Office Practices}}, {\S} 313.2 {{Works That Lack Human Authorship}}},
  author = {{U.S. Copyright Office}},
  year = {2021},
  month = jan,
  pages = {68--69},
  institution = {U.S. Copyright Office},
  abstract = {The U.S. Copyright Office will not register works produced by nature, animals, or plants. Likewise, the Office cannot register a work purportedly created by divine or supernatural beings, although the Office may register a work where the application or the deposit copy(ies) state that the work was inspired by a divine spirit.},
  langid = {american}
}

@misc{u.s.copyrightofficeUSCopyrightOffice2023,
  title = {U.{{S}}. {{Copyright Office Fair Use Index}}},
  author = {{U.S. Copyright Office}},
  year = {2023},
  journal = {Copyright Office},
  howpublished = {https://www.copyright.gov/fair-use}
}

@article{wardBrainDrainMere2017,
  title = {Brain {{Drain}}: {{The Mere Presence}} of {{One}}'s {{Own Smartphone Reduces Available Cognitive Capacity}}},
  author = {Ward, Adrian F. and Duke, Kristen and Gneezy, Ayelet and Bos, Maarten W.},
  year = {2017},
  month = apr,
  journal = {Journal of the Association for Consumer Research},
  volume = {2},
  pages = {140--154},
  doi = {10.1086/691462},
  urldate = {2023-10-03}
}

@misc{WitchHuntFears,
  title = {Witch {{Hunt Fears}}, {{Triggers}}, {{And Scapegoats}}},
  journal = {Salem Witch Museum},
  urldate = {2023-11-16},
  abstract = {Submit what you think was a fear, trigger, or scapegoat from the time of 1692. We'll review submissions, and post them to this page so you can see how specific elements impacted society, and furthermore how they have impacted the witch trials.},
  howpublished = {https://salemwitchmuseum.com/witch-hunt/},
  langid = {american},
  file = {/Users/sergiogabardo/Zotero/storage/8ISUTCFB/witch-hunt.html}
}

@misc{zhangLowPrecisionStochasticGradient2022,
  title = {Low-{{Precision Stochastic Gradient Langevin Dynamics}}},
  author = {Zhang, Ruqi and Wilson, Andrew Gordon and De Sa, Christopher},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2206.09909},
  urldate = {2024-12-04},
  abstract = {While low-precision optimization has been widely used to accelerate deep learning, low-precision sampling remains largely unexplored. As a consequence, sampling is simply infeasible in many large-scale scenarios, despite providing remarkable benefits to generalization and uncertainty estimation for neural networks. In this paper, we provide the first study of low-precision Stochastic Gradient Langevin Dynamics (SGLD), showing that its costs can be significantly reduced without sacrificing performance, due to its intrinsic ability to handle system noise. We prove that the convergence of low-precision SGLD with full-precision gradient accumulators is less affected by the quantization error than its SGD counterpart in the strongly convex setting. To further enable low-precision gradient accumulators, we develop a new quantization function for SGLD that preserves the variance in each update step. We demonstrate that low-precision SGLD achieves comparable performance to full-precision SGLD with only 8 bits on a variety of deep learning tasks.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{zouSubsampledStochasticVarianceReduced2018,
  title = {Subsampled {{Stochastic Variance-Reduced Gradient Langevin Dynamics}}},
  author = {Zou, Difan and Xu, Pan and Gu, Quanquan},
  year = {2018},
  journal = {34th Conference on Uncertainty in Artificial Intelligence},
  urldate = {2024-01-01}
}
